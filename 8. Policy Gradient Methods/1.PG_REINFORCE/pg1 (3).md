# Policy Gradient Methods 

wonseok Jung

---
## 1. Two methods of choosing action
- action-value : 
	- Learning the action value 
	- <span style="color:blue">Estimate action value을 바탕으로 action을 선택한다. </span>
	- Policies would not even exist without the action-value estimates
- Parameterized policy :
	- select actions without consulting value function
	- Value function still be used to learn policy parameter
	- <span style="color:red">Value function이 action을 선택하는 기준으로 사용되지 않는다</span>
	
---

## 1.1 Policy parameter vector

- $\theta\in R^{d'}$ :Policy's parameter vector

- $\pi(a\mid s,\theta)=Pr \left \{ At=a \mid S_t=S,  \theta_t = \theta \right \}$

	- Probability action a is taken at time t, given that the environment is in state s at time t with parameter $\theta$

---

## 1.2 Learning policy parameter



<center><img src="https://user-images.githubusercontent.com/11300712/40633437-c1403c9a-632a-11e8-9929-3b23debeacda.gif" weight="200", height="50"></center>

- $\nabla J (\theta_t)$ : stochastic estimate
	- expectation approximates the gradient of the measure to its argument $\theta$
	
---



- Policy gradient methods : optimizing parametrized policies with respect to the expected return  by gradient ascent

- Actor-critic methods : Methods that learng approximations to both policy and value function 

![pg1](https://user-images.githubusercontent.com/11300712/40636229-062dd57a-6339-11e8-9497-280cbae7cef7.JPG)

---


## 2. Policy Approximation and ITS Advantages


---

# 3. The Policy gradient Theorem

- $\epsilon-greedy$ 는 작은 action value의 변화에도 action 선택이 완전히 바뀔수 있다. 

- 하지만 policy parameterization 방법은 Parameter를 배우며 Policy parameterd의 action probabilities가 smoothly하게 변한다. 

- 이렇게 parameter에 의해 policy가 달라진다면, policy-gradient 방법을 이용하여 gradient asecent를 approximate하는것이 가능하다. 

---

# 3.1 Two type of cases


- $J(\theta)$ : Performance measure 
- Episodic case, Continuous case 두 가지로 나눌수 있다. 

- Episodic case : the performace measure as the value o the start state of the episode, 
$$J(\theta)\doteq v_{\pi \theta}(s_0)$$

- $v_{\pi \theta}(s_0)$ : True value for $\pi_{\theta}$ , the policy determinded by $\theta$



---

# 3.2 Challenging 

- With function approximation, it may seem challenging to change the policy parameter in a way that esnures improvements. 
	- performance depends on both the action selection and the distribution of states in which those selections are made
	- Both of these are affected by the policy parameter

- Policy gradient theorem : analytic expression for the gradient of performace with respect to the policy parameter

$$\bigtriangledown J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s,a) \bigtriangledown\pi(a \mid s, \theta)$$


---

# 3.3 Policy Gradient

$$\bigtriangledown J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s,a) \bigtriangledown\pi(a \mid s, \theta)$$

$\pi$ : policy corresponding to parameter vector $\theta$ 
$\propto$ : propotional to 
$\mu$ : on-policy distribuion under

- From chapter 10 : 
	- $\mu_{\pi}(s)$ :$\doteq lim_{t\rightarrow \infty} Pr\left \{ {S_t=s \mid A_{0:t-1} \sim \pi} \right \}$ 
	- steady-state distribution 

---


# 4. REINFORCE: Monte Carlo Policy Gradient

$$\bigtriangledown J(\theta) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s,a) \bigtriangledown\pi(a \mid s, \theta)$$


$$=E_{\pi}[\sum_a q_{\pi} (S_t,a) \triangledown  \pi(a \mid S_{t},\theta)]$$

- Policy gradiet theorem : sum over a states weight by how often the states occur under the target poicy $\pi$



---

# Replacing $a$ with the sample action $A_t$. 



![bandicam 2018-05-31 12-54-58-202](https://user-images.githubusercontent.com/11300712/40760473-ea8ec19a-64d1-11e8-854f-aba8684bfe2f.jpg)

- $G_t$ : Return 



---

# REINFORCE algorithm 

- Updte Rule :
$$ \theta_{t+1} \doteq \theta_t + \alpha G_t \frac{\triangledown \pi(A_t \mid S_t,\theta_t)}{\pi(A_t \mid S_t, \theta_t)} $$

- REINFORCE uses the comple te return from time t. 
- All future rewards update until the end of the epsiode. 
- <span style="color:blue">REINFORCE는 MonteCalo 알고리즘을 사용한다. 모든 업데이트는 episode가 끝난뒤 이루어진다. </span>


---
# REINFORCE-Monte Carlo pseudocode 

![bandicam 2018-05-31 13-59-53-960](https://user-images.githubusercontent.com/11300712/40762235-e9252bf6-64da-11e8-8225-3d20d064eefc.jpg)


---

![bandicam 2018-05-31 14-11-39-692](https://user-images.githubusercontent.com/11300712/40762665-8f74a3d2-64dc-11e8-8aaa-8959dd0e9cb5.jpg)

- $\alpha$ : step size에 따른 Total reward의 차이 비교

---










